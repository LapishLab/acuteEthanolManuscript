{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49db585b-f195-4343-b857-b275259b3efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df55cc50-a116-4dd1-a71b-aea47245b80c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kgrac\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import pandas as pd\n",
    "import io\n",
    "# Make NumPy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeaa391-5771-46ae-a39f-07dafa8ad768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#name intake 'inp' and pfc [etoh] 'obs'\n",
    "inp = pd.read_csv(('ncintakecomb.csv'), header=None) #intakes from MD data and published Englemann data\n",
    "obs = pd.read_csv(('etohengcomb.csv'), header=None) #MD data from our and published Engleman data\n",
    "inpEphys = pd.read_csv(('ephysitpncintake.csv'), header=None) #intakes for ensure + etoh\n",
    "inpEnsure = pd.read_csv(('enIntakeForRNN.csv'), header=None) #intakes for ensure only\n",
    "\n",
    "#reshape so rows are subjects/batches, as the RNN likes to read it\n",
    "re_inp = np.expand_dims(np.transpose(inp.copy()), axis=-1)\n",
    "re_obs = np.expand_dims(np.transpose(obs.copy()), axis=-1)\n",
    "re_inpEphys = np.expand_dims(np.transpose(inpEphys.copy()), axis=-1)\n",
    "re_inpEnsure = np.expand_dims(np.transpose(inpEnsure.copy()), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903ea39a-0de7-4169-a598-978e658426cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from tensorflow.keras import Sequential, layers\n",
    "\n",
    "nodenum = [5, 10, 20, 30] #the different node numbers to generate/test\n",
    "# Define base model function\n",
    "def create_new_model_instance(nodes):\n",
    "    model = Sequential()\n",
    "    model.add(layers.SimpleRNN(nodes, input_shape=(24, 1), return_sequences=True)) #Use SimpleRNN notation for RNN layer\n",
    "    model.add(layers.Dense(1)) #one Dense layer\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mean_absolute_error') #use MAE for the loss function\n",
    "    return model\n",
    "\n",
    "# Define the kfolds run and parameters\n",
    "# kfolds where k=5 for each node num - 50 runs per node number\n",
    "# note: this takes a LONG time to run, ~20 hours for my machine\n",
    "def perform_xval(intake_model, re_inp, re_obs):\n",
    "    num_folds = 5\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "    xval_results = []\n",
    "\n",
    "    for train_index, val_index in kf.split(re_inp):\n",
    "        inp_train, inp_val, obs_train, obs_val = train_test_split(\n",
    "            re_inp[train_index], re_obs[train_index], test_size=0.2)\n",
    "\n",
    "        # Fit the same model instance for all folds within the run\n",
    "        intake_model.fit(inp_train, obs_train, epochs=2000, batch_size=3, verbose=0)\n",
    "        \n",
    "        obs_pred = intake_model.predict(inp_val)\n",
    "        obs_pred = obs_pred.ravel()\n",
    "        obs_val = obs_val.ravel()\n",
    "\n",
    "        explained_var = explained_variance_score(obs_val, obs_pred)\n",
    "        xval_results.append(explained_var)\n",
    "\n",
    "    return xval_results\n",
    "\n",
    "num_runs = 50\n",
    "kfoldsruns = np.empty((num_runs, len(nodenum), 2), dtype=object)\n",
    "\n",
    "# iterate through runs\n",
    "for run in range(num_runs):\n",
    "    for idx, nodes in enumerate(nodenum):\n",
    "        # Create a new instance to ensure fresh weights\n",
    "        current_model = create_new_model_instance(nodes)\n",
    "        \n",
    "        xval_results = perform_xval(current_model, re_inp, re_obs) #this is where it actually does the kfolds\n",
    "        avg_explained_var = np.mean(xval_results)\n",
    "        \n",
    "        # Cache model instance and explained variance for each node number\n",
    "        kfoldsruns[run, idx, 0] = avg_explained_var\n",
    "        kfoldsruns[run, idx, 1] = current_model  # Store the current_model instance\n",
    "\n",
    "    progress_percentage = (run + 1) / num_runs * 100\n",
    "    print(f\"Run {run + 1}/{num_runs} completed ({progress_percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9f9169-3c29-47f3-976d-2dd674902e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs, num_nodes, _ = kfoldsruns.shape\n",
    "\n",
    "# Create an array to store the sorted indices for each run and node\n",
    "sorted_indices = np.argsort(kfoldsruns[:, :, 0], axis=0)[::-1]\n",
    "\n",
    "# Create empty arrays to store sorted results\n",
    "sorted_kfoldsruns = np.empty_like(kfoldsruns)\n",
    "sorted_model_instances_list = []\n",
    "\n",
    "# Iterate through nodes\n",
    "for node_idx in range(num_nodes):\n",
    "    # Iterate through runs and use sorted indices to rearrange kfoldsruns\n",
    "    for run in range(num_runs):\n",
    "        sorted_run_idx = sorted_indices[run, node_idx]\n",
    "        \n",
    "        # Sort kfoldsruns based on explained variance\n",
    "        sorted_kfoldsruns[run, node_idx] = kfoldsruns[sorted_run_idx, node_idx]\n",
    "\n",
    "        # Extract and store model instance directly from kfoldsruns\n",
    "        sorted_model_instances_list.append(kfoldsruns[sorted_run_idx, node_idx, 1])\n",
    "\n",
    "# Display the sorted results\n",
    "for node_idx in range(num_nodes):\n",
    "    print(f\"\\nNode {nodenum[node_idx]} - Sorted Results:\")\n",
    "    for run in range(num_runs):\n",
    "        avg_explained_var = sorted_kfoldsruns[run, node_idx, 0]\n",
    "        model_instance = sorted_model_instances_list[node_idx * num_runs + run]\n",
    "        print(f\"Run {run + 1}: Explained Variance = {avg_explained_var:.4f}, Model Instance: {model_instance}\")\n",
    "\n",
    "top_10_models_data_node_20 = list(zip(sorted_kfoldsruns[:, 2, 0][:10], sorted_kfoldsruns[:, 2, 1][:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77c94c6-79b7-43a9-843c-ae216fc97617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the kfoldruns and model instances to a workspace to be used for intake only runs\n",
    "import pickle\n",
    "\n",
    "kfoldsworkspace = {\n",
    "    'kfoldsruns': kfoldsruns,\n",
    "    'best20nodemodels': top_10_models_data_node_20\n",
    "}\n",
    "\n",
    "# Save the workspace to a file name\n",
    "with open('kfolds_workspace.pkl', 'wb') as file:\n",
    "    pickle.dump(kfoldsworkspace, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
